{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvMapNetDataset(Dataset):\n",
    "    def __init__(self, images_path, type, minibatch_size, *, force_len=None):\n",
    "        self.__images_path = os.path.join(images_path, type)\n",
    "        self.__images = os.listdir(self.__images_path)\n",
    "\n",
    "        self.__force_len = None\n",
    "        if force_len:\n",
    "            self.__force_len = force_len\n",
    "        else:\n",
    "            self.__force_len = len(self.__images)\n",
    "\n",
    "        if self.__force_len % minibatch_size != 0:\n",
    "            self.__force_len += minibatch_size - (self.__force_len % minibatch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.__force_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.__images_path, self.__images[index % len(self.__images)])\n",
    "        data = np.load(image_path, allow_pickle=True)\n",
    "        data = torch.from_numpy(np.array([data[:,:,0], data[:,:,1], data[:,:,2]]))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L1Loss, self).__init__()\n",
    "\n",
    "    def forward(self, orig, pred, mask):\n",
    "        return (orig * mask - pred * mask).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(L2Loss, self).__init__()\n",
    "\n",
    "    def forward(self, orig, pred):\n",
    "        return (orig - pred).norm().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeRealLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FakeRealLoss, self).__init__()\n",
    "\n",
    "    def forward(self, orig, pred):\n",
    "        return -((pred + 1e-9).log() + (1 - orig + 1e-9).log()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdversarialLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred):\n",
    "        return -(pred + 1e-9).log().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ORBPatcher:\n",
    "    def __init__(self, *, key_points=1000):\n",
    "        self.__orb = cv2.ORB_create(key_points)\n",
    "\n",
    "    def __call__(self, image):\n",
    "        key_points, _ = self.__orb.detectAndCompute(image, None)\n",
    "\n",
    "        res = np.zeros((image.shape[0], image.shape[1]), dtype=np.int)\n",
    "        for point in key_points:\n",
    "            x, y = map(np.int, point.pt)\n",
    "            res[y, x] = 1\n",
    "\n",
    "        res == 1\n",
    "        return res.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansFiles:\n",
    "    def __init__(self, orb, *, clusters=5, n_iter=300, partitions=1):\n",
    "        self.__orb = orb\n",
    "        self.__clusters = clusters\n",
    "        self.__n_iter = n_iter\n",
    "        self.__partitions = partitions\n",
    "\n",
    "        self.__centers = None\n",
    "\n",
    "        self.__centers_save_path = os.path.join('KMeans','k_means.bin')\n",
    "\n",
    "    def fit(self, files):\n",
    "        partition = (len(files) + self.__partitions - 1) // self.__partitions\n",
    "        for num in range(self.__partitions):\n",
    "            print(f'preparing partition {num + 1}')\n",
    "            tmp = self.__prepare_partition(files[num::self.__partitions])\n",
    "            torch.save(tmp, os.path.join(f'{num}.pt'))\n",
    "            print(f'partition {num + 1} prepared\\n')\n",
    "        print('partitions prepared\\n')\n",
    "\n",
    "        if self.__partitions > 1:\n",
    "            for iteration in range(self.__n_iter):\n",
    "                print(f'fitting iteration {iteration + 1}')\n",
    "                for num in range(self.__partitions):\n",
    "                    tmp = torch.load(os.path.join(f'{num}.pt')).to(device)\n",
    "                    print(f'partition {num + 1} loaded')\n",
    "                    self.__fit_partition(tmp)\n",
    "                    del tmp\n",
    "                    torch.cuda.empty_cache()\n",
    "                    print(f'partition {num + 1} fitted')\n",
    "                print(f'iteration {iteration + 1} fitted\\n')\n",
    "                if (iteration + 1) % 10 == 0:\n",
    "                    self.save()\n",
    "                    print('model saved\\n')\n",
    "                print()\n",
    "        else:\n",
    "            tmp = torch.load(os.path.join('0.pt')).to(device)\n",
    "            print(f'partition 0 loaded')\n",
    "            for iteration in range(self.__n_iter):\n",
    "                print(f'fitting iteration {iteration + 1}')\n",
    "                self.__fit_partition(tmp)\n",
    "                print(f'iteration {iteration + 1} fitted\\n')\n",
    "                torch.cuda.empty_cache()\n",
    "                if (iteration + 1) % 10 == 0:\n",
    "                    self.save()\n",
    "                    print('model saved\\n')\n",
    "                print()\n",
    "\n",
    "        self.save()\n",
    "        print('k-means ready')\n",
    "\n",
    "        for num in range(self.__partitions):\n",
    "            os.remove(os.path.join('{}.pt'.format(num)))\n",
    "\n",
    "        self.__centers = self.__centers.to(device)\n",
    "\n",
    "    def __prepare_partition(self, files):\n",
    "        return torch.tensor([\n",
    "            self.__orb((((np.load(file)) + 1) / 2 * 255).astype(np.uint8))\n",
    "            for file in files\n",
    "        ])\n",
    "\n",
    "    def __fit_partition(self, orig):\n",
    "        x = orig * 1.0\n",
    "\n",
    "        n, d = x.shape\n",
    "        if self.__centers is None:\n",
    "            self.__centers = x[:self.__clusters, :].clone() * 1.0\n",
    "\n",
    "        x_i = x.view(n, 1, d)\n",
    "        c_j = self.__centers.view(1, self.__clusters, d)\n",
    "\n",
    "        D_ij = ((x_i - c_j) ** 2).sum(-1)\n",
    "        clusters = D_ij.argmin(dim=1).long().view(-1)\n",
    "\n",
    "        self.__centers.zero_()\n",
    "        self.__centers.scatter_add_(0, clusters[:, None].repeat(1, d), x)\n",
    "\n",
    "        Ncl = torch.bincount(clusters, minlength=self.__clusters)\n",
    "        Ncl = Ncl.type_as(self.__centers)\n",
    "        Ncl = Ncl.view(self.__clusters, 1)\n",
    "        self.__centers /= Ncl\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.__centers == None:\n",
    "            print('k-means wasn\\'t fitted')\n",
    "            return\n",
    "\n",
    "        n, d = x.shape\n",
    "        x_i = x.view(n, 1, d)\n",
    "        c_j = self.__centers.view(1, self.__clusters, d)\n",
    "        D_ij = ((x_i - c_j) ** 2).sum(-1)\n",
    "        clusters = D_ij.argmin(dim=1).long().view(-1)\n",
    "\n",
    "        res = torch.zeros((n, self.__clusters), requires_grad=False).to(device)\n",
    "        for num, item in enumerate(clusters):\n",
    "            res[num, item] = 1\n",
    "        return res\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.__centers, self.__centers_save_path)\n",
    "\n",
    "    def load(self):\n",
    "        self.__centers = torch.load(self.__centers_save_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterLoss(nn.Module):\n",
    "    def __init__(self, orb, minibatch_size, *, train_k_means=False, partitions=1, clusters=5):\n",
    "        super(ClusterLoss, self).__init__()\n",
    "        self.__orb = orb\n",
    "        self.__minibatch_size = minibatch_size\n",
    "        self.__clusters = clusters\n",
    "\n",
    "        self.__k_means = KMeansFiles(orb, clusters=clusters, partitions=partitions)\n",
    "        if train_k_means:\n",
    "            self.__k_means.fit(self.__prepare_data())\n",
    "            self.__k_means.save()\n",
    "        else:\n",
    "            self.__k_means.load()\n",
    "\n",
    "    def forward(self, images, pred):\n",
    "        if device != 'cpu':\n",
    "            images = images.detach().cpu().numpy()\n",
    "        else:\n",
    "            images = images.detach().numpy()\n",
    "\n",
    "        images = np.array([\n",
    "            np.concatenate([\n",
    "                image[0].reshape(*image[0].shape, 1),\n",
    "                image[1].reshape(*image[1].shape, 1),\n",
    "                image[2].reshape(*image[2].shape, 1),\n",
    "            ], axis=2) for image in images\n",
    "        ])\n",
    "        \n",
    "        base = self.__k_means.predict(torch.tensor([\n",
    "            self.__orb(((image + 1) * 255 / 2).astype(np.uint8))\n",
    "            for image in images\n",
    "        ]).to(device))\n",
    "\n",
    "        return -(base * torch.log(nn.Softmax(dim=1)(pred.view(self.__minibatch_size, self.__clusters)) + 1e-9)).sum()\n",
    "\n",
    "    def __prepare_data(self):\n",
    "        catalogs = [\n",
    "            os.path.join('LavalIndoorHDRDatasetReadySmall', 'train'),\n",
    "            os.path.join('PanoIndoorLDRDatasetReadySmall', 'test'),\n",
    "            os.path.join('LavalIndoorHDRDatasetReadySmall', 'train'),\n",
    "            os.path.join('PanoIndoorLDRDatasetReadySmall', 'test')\n",
    "        ]\n",
    "        files = []\n",
    "        for catalog in catalogs:\n",
    "            files.extend([os.path.join(catalog, file) for file in os.listdir(catalog)])\n",
    "        return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLoss(nn.Module):\n",
    "    def __init__(self, minibatch_size, *, generate_masks=False, masks_count=50, base_shape=(128, 256)):\n",
    "        super(ProjectionLoss, self).__init__()\n",
    "        self.__minibatch_size = minibatch_size\n",
    "        self.__masks_path = 'Masks'\n",
    "\n",
    "        if generate_masks:\n",
    "            self.__masks = [\n",
    "                self.__build_mask(base_shape)\n",
    "                for _ in range(masks_count)\n",
    "            ]\n",
    "            for num, mask in enumerate(self.__masks):\n",
    "                path = os.path.join(self.__masks_path, str(num + 1))\n",
    "                np.save(path, mask)\n",
    "            self.__masks = torch.from_numpy(np.array(self.__masks)).to(device)\n",
    "        else:\n",
    "            self.__masks = []\n",
    "            for i in range(masks_count):\n",
    "                path = os.path.join(self.__masks_path, str(i + 1) + '.npy')\n",
    "                self.__masks.append(np.load(path))\n",
    "            self.__masks = torch.from_numpy(np.array(self.__masks)).to(device)\n",
    "\n",
    "    def forward(self, orig, pred):\n",
    "        res = None\n",
    "\n",
    "        for mask in self.__masks:\n",
    "            if not res:\n",
    "                res = torch.abs((orig * mask).sum() - (pred * mask).sum())\n",
    "            else:\n",
    "                res += torch.abs((orig * mask).sum() - (pred * mask).sum())\n",
    "\n",
    "        return res\n",
    "\n",
    "    def __build_mask(self, base_shape):\n",
    "        base_shape = (*base_shape, 3)\n",
    "        image = np.zeros(base_shape)\n",
    "\n",
    "        for _ in range(int(random.gauss(4, 0.7))):\n",
    "            scale = random.randint(10, 40)\n",
    "\n",
    "            h, w, _ = base_shape\n",
    "            h = h * scale // 100\n",
    "            w = w * scale // 100\n",
    "\n",
    "            tmp = np.ones((h, w, 3)) * 255\n",
    "\n",
    "            angle = random.randint(0, 180)\n",
    "\n",
    "            M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1)\n",
    "            tmp = cv2.warpAffine(tmp, M, (w, h))\n",
    "\n",
    "            h_c = random.randint(base_shape[0] // 3, 2 * base_shape[0] // 3)\n",
    "            w_c = random.randint(base_shape[1] // 3, 2 * base_shape[1] // 3)\n",
    "\n",
    "            top_pad = base_shape[0] - h_c - h // 2\n",
    "            bottom_pad = h_c - h // 2 - h % 2\n",
    "\n",
    "            right_pad = base_shape[1] - w_c - w // 2\n",
    "            left_pad = w_c - w // 2 - w % 2\n",
    "\n",
    "            tmp = np.pad(tmp, ((top_pad, bottom_pad), (left_pad, right_pad), (0, 0)))\n",
    "\n",
    "            image = cv2.add(image, tmp)\n",
    "\n",
    "        res = (image == 0).astype(np.int)\n",
    "        res = np.array([res[:,:,0], res[:,:,1], res[:,:,2]])\n",
    "\n",
    "        return [res.copy() for _ in range(self.__minibatch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvMapNetLoss(nn.Module):\n",
    "    def __init__(self, orb, minibatch_size, *, train_k_means=False, generate_masks=False):\n",
    "        super(EnvMapNetLoss, self).__init__()\n",
    "        self.L1Loss = L1Loss()\n",
    "        self.L2Loss = L2Loss()\n",
    "        self.AdversarialLoss = AdversarialLoss()\n",
    "        self.ClusterLoss = ClusterLoss(orb, minibatch_size, train_k_means=train_k_means)\n",
    "        self.ProjectionLoss = ProjectionLoss(minibatch_size, generate_masks=generate_masks)\n",
    "\n",
    "    def forward(self, orig, mask, pred, discriminator_pred1, discriminator_pred2):\n",
    "        return\\\n",
    "            0.5 * self.L1Loss(orig, pred, mask) +\\\n",
    "            0.01 * self.L2Loss(orig, pred) +\\\n",
    "            self.ProjectionLoss(orig, pred) +\\\n",
    "            self.AdversarialLoss(discriminator_pred1) +\\\n",
    "            self.ClusterLoss(orig, discriminator_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorLoss(nn.Module):\n",
    "    def __init__(self, orb, minibatch_size, *, train_k_means=False):\n",
    "        super(DiscriminatorLoss, self).__init__()\n",
    "        self.FakeRealLoss = FakeRealLoss()\n",
    "        self.ClusterLoss = ClusterLoss(orb, minibatch_size, train_k_means=train_k_means)\n",
    "\n",
    "    def forward(self, orig, pred_1, pred_2, pred_g):\n",
    "        return\\\n",
    "            self.FakeRealLoss(pred_1, pred_g) +\\\n",
    "            self.ClusterLoss(orig, pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvMapNetConvBlock(nn.Module):\n",
    "    def __init__(self, channels, *, sets_count=5, conv_channels=16):\n",
    "        super(EnvMapNetConvBlock, self).__init__()\n",
    "        self.__blocks = nn.Sequential(*[\n",
    "            item for sublist in [\n",
    "                self.__get_set(channels, conv_channels, i)\n",
    "                for i in range(sets_count)\n",
    "            ] for item in sublist\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        short_cut = x\n",
    "        counter = 0\n",
    "        for block in self.__blocks:\n",
    "            x = block(x)\n",
    "            counter += 1\n",
    "            if counter == 3:\n",
    "                x = torch.cat((x, short_cut), 1)\n",
    "                short_cut = x\n",
    "                counter = 0\n",
    "        return x\n",
    "    \n",
    "    def __get_set(self, in_channels, conv_channels, i):\n",
    "        channels = in_channels + conv_channels * i\n",
    "        return [\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Conv2d(\n",
    "                channels,\n",
    "                conv_channels,\n",
    "                kernel_size=(3, 3),\n",
    "                padding=(1, 1),\n",
    "                padding_mode='circular',\n",
    "            )\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvMapNetDownsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EnvMapNetDownsampleBlock, self).__init__()\n",
    "        self.__conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=(1, 1),\n",
    "            padding_mode='circular',\n",
    "        )\n",
    "        self.__downsample = nn.AvgPool2d((2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.__conv(x)\n",
    "        x = self.__downsample(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvMapNetUpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EnvMapNetUpsampleBlock, self).__init__()\n",
    "        self.__conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=(1, 1),\n",
    "            padding_mode='circular',\n",
    "        )\n",
    "        self.__upsample = nn.Upsample(scale_factor=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.__upsample(x)\n",
    "        x = self.__conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dk_orig, uk_orig, *, sets_count=5,\n",
    "                 conv_channels=16, neck_channels=64):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        dk = [3] + dk_orig\n",
    "        self.__downsampling_blocks = nn.Sequential(*[\n",
    "            item for sublist in [\n",
    "                [\n",
    "                    EnvMapNetConvBlock(\n",
    "                        dk[i - 1],\n",
    "                        sets_count=sets_count,\n",
    "                        conv_channels=conv_channels\n",
    "                    ),\n",
    "                    EnvMapNetDownsampleBlock(\n",
    "                        dk[i - 1] + sets_count * conv_channels,\n",
    "                        dk[i]\n",
    "                    )\n",
    "                ]\n",
    "                for i in range(1, len(dk))\n",
    "            ] for item in sublist\n",
    "        ])\n",
    "\n",
    "        self.__neck = nn.Conv2d(\n",
    "            dk[-1],\n",
    "            neck_channels,\n",
    "            kernel_size=(1, 1),\n",
    "        )\n",
    "\n",
    "        uk = [neck_channels] + uk_orig\n",
    "        self.__upsampling_blocks = nn.Sequential(*[\n",
    "            item for sublist in [\n",
    "                [\n",
    "                    EnvMapNetUpsampleBlock(\n",
    "                        uk[i - 1] + (i != 1) *\n",
    "                            (sets_count * conv_channels),\n",
    "                        uk[i]),\n",
    "                    EnvMapNetConvBlock(\n",
    "                        uk[i],\n",
    "                        sets_count=sets_count,\n",
    "                        conv_channels=conv_channels\n",
    "                    )\n",
    "                ]\n",
    "                for i in range(1, len(uk))\n",
    "            ] for item in sublist\n",
    "        ])\n",
    "\n",
    "        self.__out = nn.Conv2d(\n",
    "            uk[-1] + sets_count * conv_channels,\n",
    "            3,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=(1, 1),\n",
    "            padding_mode='circular',\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.__downsampling_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.__neck(x)\n",
    "\n",
    "        for block in self.__upsampling_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.__out(x)\n",
    "\n",
    "        return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, *, sets_count=2):\n",
    "        super(DiscriminatorResidualBlock, self).__init__()\n",
    "        self.__avg = nn.AvgPool2d((2, 2))\n",
    "        self.__conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=(3, 3),\n",
    "            padding=(1, 1),\n",
    "            padding_mode='circular',\n",
    "        )\n",
    "\n",
    "        self.__blocks = nn.Sequential(*[\n",
    "            item for sublist in [\n",
    "                self.__get_set(in_channels if i == 0 else out_channels, out_channels)\n",
    "                for i in range(sets_count)\n",
    "            ] for item in sublist\n",
    "        ])\n",
    "\n",
    "    def forward(self, inp):\n",
    "        sc = self.__avg(inp)\n",
    "        sc = self.__conv(sc)\n",
    "        sc = nn.functional.pad(\n",
    "            sc, (sc.shape[-1] // 2, sc.shape[-1] // 2, sc.shape[-2] // 2, sc.shape[-2] // 2)\n",
    "        )\n",
    "\n",
    "        x = inp\n",
    "        for block in self.__blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return x + sc\n",
    "    \n",
    "    def __get_set(self, in_channels, out_channels):\n",
    "        return [\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=(3, 3),\n",
    "                padding=(1, 1),\n",
    "                padding_mode='circular',\n",
    "            )\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ak_orig, *, sets_count=2, base_shape=(128, 256), clusters=5):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        ak = [3] + ak_orig\n",
    "\n",
    "        self.__blocks = nn.Sequential(*[\n",
    "            DiscriminatorResidualBlock(ak[i - 1], ak[i], sets_count=sets_count)\n",
    "            for i in range(1, len(ak))\n",
    "        ])\n",
    "        \n",
    "        self.__out1_conv = nn.Conv2d(\n",
    "            ak[-1],\n",
    "            1,\n",
    "            kernel_size=base_shape,\n",
    "        )\n",
    "        self.__out2_conv = nn.Conv2d(\n",
    "            ak[-1],\n",
    "            clusters,\n",
    "            kernel_size=base_shape,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.__blocks:\n",
    "            x = block(x)\n",
    "        return nn.Sigmoid()(self.__out1_conv(x)), self.__out2_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk = [64, 128, 128, 128, 256, 256, 512]\n",
    "uk = [512, 256, 256, 128, 128, 128, 64]\n",
    "ak = [64, 128, 256, 256, 256, 256, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size = 4\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = nn.DataParallel(Generator(dk, uk)).to(device)\n",
    "if os.path.exists(os.path.join('generator.bin')):\n",
    "    model.load_state_dict(torch.load(os.path.join('generator.bin')))\n",
    "discriminator = nn.DataParallel(Discriminator(ak)).to(device)\n",
    "if os.path.exists(os.path.join('discriminator.bin')):\n",
    "    model.load_state_dict(torch.load(os.path.join('discriminator.bin')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_train_dataloader = DataLoader(\n",
    "    EnvMapNetDataset(os.path.join('LavalIndoorHDRDatasetReadySmall'), 'train', minibatch_size), batch_size=minibatch_size, shuffle=True\n",
    ")\n",
    "generator_test_dataloader = DataLoader(\n",
    "    EnvMapNetDataset(os.path.join('LavalIndoorHDRDatasetReadySmall'), 'test', minibatch_size), batch_size=minibatch_size, shuffle=True\n",
    ")\n",
    "discriminator_train_dataloader = DataLoader(\n",
    "    EnvMapNetDataset(os.path.join('PanoIndoorLDRDatasetReadySmall'), 'train', minibatch_size, force_len=len(generator_train_dataloader.dataset)), batch_size=minibatch_size, shuffle=True\n",
    ")\n",
    "discriminator_test_dataloader = DataLoader(\n",
    "    EnvMapNetDataset(os.path.join('PanoIndoorLDRDatasetReadySmall'), 'test', minibatch_size, force_len=len(generator_test_dataloader.dataset)), batch_size=minibatch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "orb = ORBPatcher()\n",
    "generator_loss = EnvMapNetLoss(orb, minibatch_size)\n",
    "discriminator_loss = DiscriminatorLoss(orb, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimiser = torch.optim.Adam(generator.parameters(), lr=0.0002)\n",
    "discriminator_optimiser = torch.optim.Adam(discriminator.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.zeros((minibatch_size, 3, 128, 256)).to(device)\n",
    "to_change = mask[:,:,128 // 3:2 * 128 // 3, 256 // 3:2 * 256 // 3]\n",
    "mask[:,:,128 // 3:2 * 128 // 3, 256 // 3:2 * 256 // 3] = torch.ones_like(to_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(generator,\n",
    "               generator_train_dataloader,\n",
    "               generator_loss,\n",
    "               generator_optimiser,\n",
    "               discriminator,\n",
    "               discriminator_train_dataloader,\n",
    "               discriminator_loss,\n",
    "               discriminator_optimiser\n",
    "              ):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    g_size = len(generator_train_dataloader.dataset)\n",
    "    d_size = len(discriminator_train_dataloader.dataset)\n",
    "\n",
    "    for batch, (g_data, d_data) in enumerate(zip(\n",
    "        generator_train_dataloader,\n",
    "        discriminator_train_dataloader\n",
    "    )):\n",
    "\n",
    "        g_data = g_data.to(device)\n",
    "        rand = torch.zeros_like(g_data).to(device).uniform_(-1, 1) * (1 - mask)\n",
    "        to_apply = (g_data * mask + rand).float()\n",
    "        g_pred = generator(to_apply)\n",
    "\n",
    "        discriminator.eval()\n",
    "        d_pred1, d_pred2 = discriminator(g_pred)\n",
    "        discriminator.train()\n",
    "\n",
    "        g_loss = generator_loss(g_data, mask, g_pred, d_pred1, d_pred2)\n",
    "        generator_optimiser.zero_grad()\n",
    "        g_loss.backward()\n",
    "        generator_optimiser.step()\n",
    "\n",
    "        if batch > 0 and batch % 10 == 0:\n",
    "            loss, current = g_loss.item(), batch * len(g_data)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{g_size:>5d}]\", end='\\t')\n",
    "\n",
    "        del g_data, rand, to_apply, g_pred, d_pred1, d_pred2, g_loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        d_data = d_data.float().to(device)\n",
    "        \n",
    "        generator.eval()\n",
    "        rand = torch.zeros_like(d_data).to(device).uniform_(-1, 1) * (1 - mask)\n",
    "        to_apply = (d_data * mask + rand).float()\n",
    "        g_pred = generator(to_apply)\n",
    "        generator.train()\n",
    "\n",
    "        d_pred1, d_pred2 = discriminator(d_data)\n",
    "        g_pred1, g_pred2 = discriminator(g_pred)\n",
    "\n",
    "        d_loss = discriminator_loss(d_data, d_pred1, d_pred2, g_pred1)\n",
    "\n",
    "        discriminator_optimiser.zero_grad()\n",
    "        d_loss.backward()\n",
    "        discriminator_optimiser.step()\n",
    "\n",
    "        if batch > 0 and batch % 10 == 0:\n",
    "            loss, current = d_loss.item(), batch * len(d_data)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{d_size:>5d}]\")\n",
    "\n",
    "        del d_data, d_pred1, d_pred2, d_loss, g_pred, g_pred1, g_pred2, rand, to_apply\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def test_loop(generator,\n",
    "              generator_test_dataloader,\n",
    "              generator_loss,\n",
    "              discriminator,\n",
    "              discriminator_test_dataloader,\n",
    "              discriminator_loss\n",
    "             ):\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "\n",
    "    g_size = len(generator_test_dataloader.dataset)\n",
    "    d_size = len(discriminator_test_dataloader.dataset)\n",
    "    g_loss, d_loss = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for g_data, d_data in zip(\n",
    "            generator_test_dataloader,\n",
    "            discriminator_test_dataloader\n",
    "        ):\n",
    "\n",
    "            g_data = g_data.to(device)\n",
    "            rand = torch.zeros_like(g_data).to(device).uniform_(-1, 1) * (1 - mask)\n",
    "            to_apply = (g_data * mask + rand).float()\n",
    "            g_pred = generator(to_apply)\n",
    "            d_pred1, d_pred2 = discriminator(g_pred)\n",
    "            g_loss += generator_loss(g_data, mask, g_pred, d_pred1, d_pred2).item()\n",
    "\n",
    "            del g_data, rand, to_apply, g_pred, d_pred1, d_pred2\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            d_data = d_data.to(device).float()\n",
    "            rand = torch.zeros_like(d_data).to(device).uniform_(-1, 1) * (1 - mask)\n",
    "            to_apply = (d_data * mask + rand)\n",
    "            g_pred = generator(to_apply)\n",
    "\n",
    "            d_pred1, d_pred2 = discriminator(d_data)\n",
    "            g_pred1, g_pred2 = discriminator(g_pred)\n",
    "\n",
    "            d_loss += discriminator_loss(d_data, d_pred1, d_pred2, g_pred1).item()\n",
    "\n",
    "            del d_data, d_pred1, d_pred2, g_pred, g_pred1, g_pred2, rand, to_apply\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    g_loss /= g_size\n",
    "    d_loss /= d_size\n",
    "\n",
    "    print(\"Test Error:\")\n",
    "    print(f\" Avg generator loss: {g_loss:>8f}\")\n",
    "    print(f\" Avg discriminator loss: {d_loss:>8f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1026790.904319  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 425943.967494  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 666271.959024  [  120/ 1924]\tloss: 124.339600  [  120/ 1924]\n",
      "loss: 715800.574464  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 855999.054968  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 479575.910125  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 868989.089734  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 873286.945349  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 484448.753839  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 750198.785272  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 1128436.706616  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 308621.446960  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 327102.103103  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 677506.113343  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 730887.698041  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 2306438.948861  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 1293692.296492  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 264986.377035  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 208862.162782  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 811990.832818  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 226190.709919  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 456458.723736  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 4191862.753901  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 632632.479172  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 459977.315645  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 612440.044039  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 670994.391073  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 2226993.852293  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 541132.196534  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 2084496.000818  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 380203.178973  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 1598093.967447  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 372591.610286  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 1661932.516567  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 2820533.713466  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 8442042.144819  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 6318085.977304  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 8779982.519955  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 6601258.634525  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 5681706.562275  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 7565306.363433  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 10808659.906901  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 6555978.610562  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 7844292.632045  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 8345561.723263  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7162682.769654  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 8772594.861171  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 8953217.907834  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 1 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.123864\n",
      " Avg discriminator loss: 20.723267\n",
      "\n",
      "Epoch 1 tested\n",
      "\n",
      "Epoch 2 started\n",
      "loss: 8914586.281703  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 7231876.184712  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 9789353.525648  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 8656014.134431  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 7829461.659092  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 6824500.036070  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 10616131.447756  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7730797.355093  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 6754821.652427  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 8031007.018307  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 7188548.709651  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7530987.156369  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 6794622.935232  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 6470264.335734  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 6730192.760032  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 6331604.081623  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 6998044.527672  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 8225838.766843  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 6868123.476554  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 8485174.055903  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 6855855.525841  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7646294.780541  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 6591780.343716  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 6272527.542677  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 5638983.988839  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 6889967.246205  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7532523.159475  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 8591010.507221  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 6600881.541635  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 7625290.370063  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 7960591.399713  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 5805260.714785  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 7371762.460811  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 7451672.892110  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 6952589.340856  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 6495128.501451  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 6746711.373904  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 6080740.679140  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 9746309.493048  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 8518667.537246  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 7591701.103068  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 7876687.185159  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 7619510.130654  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 6227920.254777  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 11100392.596165  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7772634.611410  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 7848285.508136  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 6621200.542122  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 2 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.124221\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 2 tested\n",
      "\n",
      "Epoch 3 started\n",
      "loss: 5688881.841003  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 6243972.913958  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 8357730.134367  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 7473823.504893  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 7278330.562854  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 6793528.744947  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 7608886.062744  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7398311.122899  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 7548556.428985  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 8324684.700587  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 6941165.227151  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 6989531.664682  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 6071910.592277  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 6711952.377222  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 7965118.514238  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 6267106.985148  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 8440811.294027  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 7173140.411938  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 8103109.564411  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 7931535.168809  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 7364285.154383  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 8092348.936230  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 6206722.099760  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6431805.435708  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 7146694.050384  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 7769262.161744  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7445417.687695  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 6813501.412719  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 6115698.208238  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 7405847.377120  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 7012719.568784  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 8123885.485108  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 6374033.699177  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 7564304.353986  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 7492116.981513  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 6517937.662782  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 7004872.402819  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 6661295.242325  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 9305210.590771  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 12875010.735436  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 9285379.038782  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 8447474.506495  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 6659195.149282  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 7354323.557885  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 7413941.590071  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 6246696.417356  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 6225425.358903  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 6645303.063157  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 3 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.124477\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 3 tested\n",
      "\n",
      "Epoch 4 started\n",
      "loss: 8573216.538207  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 6527089.806716  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 6719615.261023  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 7291874.314260  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 7719978.385635  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 6379247.001924  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 6894888.229003  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7321227.834534  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 8514679.124824  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 7594439.519401  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 8325061.566114  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7757236.575745  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 6687010.717596  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 5960168.793498  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 6775340.603480  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 7837953.295445  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 7926814.506940  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 6900673.821197  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 7247934.970042  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 7958575.345635  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 7524132.534949  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 6006712.271935  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 8077518.899704  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 7862253.431615  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 8125848.446390  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 5472408.170230  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7060726.410050  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 7482195.435160  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 6669297.001073  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 6908788.996694  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 6473509.401911  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 6915250.322828  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 9308220.798302  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 6725257.462546  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 6074025.098653  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 7197516.106534  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 6550471.283221  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 7173571.904234  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 7588981.441839  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 7815831.239906  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 7013973.321170  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 7736446.482894  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 6395452.175405  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 9344906.744628  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 8580958.474753  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 6616621.996838  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 7433698.047237  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 7245604.857122  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 4 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.124583\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 4 tested\n",
      "\n",
      "Epoch 5 started\n",
      "loss: 6758834.297481  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 6889197.252492  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 7199886.999025  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 8421491.898312  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 6720777.837528  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 6597273.052059  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 7977930.231150  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7114282.829613  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 7373899.261081  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 11697836.582168  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 8194604.546457  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 8103063.290176  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 7688310.551285  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 7675072.327047  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 7209174.597029  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 6630679.557021  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 7064353.994338  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 8117322.310907  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 6305424.967753  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 6713767.490270  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 7046679.148073  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 6376511.346551  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 8592239.786592  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 7318861.272792  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 6778349.072303  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 7165481.832055  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7277637.305079  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 7226227.302393  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 8314623.150216  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 6016376.291996  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 7624032.288589  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 7197775.273926  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 8482926.712002  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 6872096.596366  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 9021226.230329  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 7501472.035210  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 6883696.515548  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 7407365.791280  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 6902543.140493  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 10001639.776780  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 7152779.629634  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 6603441.356897  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 7558399.633350  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 8014596.195422  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 7291347.417733  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7820325.677721  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 7698283.249285  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 9576771.410194  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 5 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.122327\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 5 tested\n",
      "\n",
      "Epoch 6 started\n",
      "loss: 7280187.912721  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 7294956.332055  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 7028217.097921  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 7106869.296592  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 6669704.452983  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 7400629.202294  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 6501842.395832  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 8716320.413377  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 7796837.082959  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 5897688.926637  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 11120956.631019  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7448517.123291  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 6462181.117401  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 6973886.364866  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 6445706.999669  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 6631204.100075  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 7716305.993094  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 5849122.026803  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 12652265.854961  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 6427213.810421  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 7429886.367341  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 6296061.828369  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 8872041.932597  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 6942870.107392  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 6827913.187647  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 10222313.441485  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 8376524.109611  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 6376355.974884  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 7688617.699316  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 6845774.405812  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 9864976.047657  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 5539256.742821  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 6888877.912041  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 7749076.493491  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 6254076.883437  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 7501909.844529  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 7729419.353403  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 8405641.263936  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 7021969.903803  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 7212916.111101  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 9946948.616470  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 6687209.045414  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 6160563.534244  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 6814578.968065  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 6294136.675825  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 6316125.418777  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 6503992.590978  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 10989787.554165  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 6 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.122804\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 6 tested\n",
      "\n",
      "Epoch 7 started\n",
      "loss: 8647748.094417  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 7206878.384580  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 6810096.288402  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 5820491.281877  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 4879555.501581  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 8945278.174922  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 6991824.759981  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7611926.264276  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 7403973.245796  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 7784010.121095  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 6869503.299831  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7340103.115999  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 6429489.343378  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 8765121.219006  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 6757399.853707  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 7912553.361092  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 7122414.067619  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 7104376.145768  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 6970967.960065  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 10278281.434236  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 6918826.558440  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7859550.804379  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 6999480.220700  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 7217234.741544  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 6395781.308265  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 7388184.746022  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7039895.302305  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 8110448.714156  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 7790584.049995  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 6968141.769938  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 7939803.711816  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 7082958.811901  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 9067208.436549  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 7755042.944384  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 5835231.152909  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 7204687.711897  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 6933572.036124  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 8291682.689384  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 6392616.071852  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 7270237.540306  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 6691488.267583  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 7075263.555298  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 8053405.345729  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 6246998.842539  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 7940735.074992  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7452891.614112  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 6450745.616040  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 7907799.089933  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 7 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.125152\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 7 tested\n",
      "\n",
      "Epoch 8 started\n",
      "loss: 6631253.935780  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 6045790.871617  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 5081528.161960  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 6205934.036920  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 7483531.837798  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 7217817.787605  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 5949272.441148  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7056830.886305  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 6928020.355492  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 7585900.727553  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 6622733.904609  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 8148501.359912  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 6851204.626442  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 7130592.400062  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 7278440.098641  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 7516877.448441  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 9253088.773197  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7261848.686744  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 7871824.579380  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 7400706.426199  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 10871810.809192  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7408459.734507  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 8175576.140449  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 6486268.107373  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 6974957.576678  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 7189967.835729  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7928795.880152  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 6858382.631690  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 6943081.812048  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 9088873.071432  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 8840730.348630  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 7301213.538495  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 7560553.256202  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 6576883.788524  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 7037946.469700  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 6379888.149292  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 7241629.649021  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 7087438.577483  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 5420523.887097  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 7205073.184513  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 7047807.978680  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 7336049.836821  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 8078509.560895  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 8520057.161588  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 6837210.537266  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7676040.575932  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 6715917.814811  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 6235711.790543  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 8 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.123652\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 8 tested\n",
      "\n",
      "Epoch 9 started\n",
      "loss: 8254868.815339  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 6365003.528379  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 7109797.356915  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 7022618.467812  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 6537418.311716  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 8435368.899689  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 7544177.387014  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 6336950.768384  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 6534123.495077  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 7949985.064806  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 6744283.662689  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7544635.360468  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 6723596.045979  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 6665837.180297  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 10393187.101185  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 8082560.194508  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 7162044.115787  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 7560890.773328  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 7772052.798731  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 6899268.878366  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 8107687.644673  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7292590.912555  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 7686139.025867  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 5992574.147394  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 8929180.625021  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 10758373.454497  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7413516.512393  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 7737170.791769  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 6351355.673566  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 8367762.611828  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 6424300.218795  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 7722787.567585  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 7908614.852857  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 8001204.265037  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 9543285.381592  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 8359490.324314  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 7983363.177750  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 5524235.030784  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 6689224.305620  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 6597823.367772  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 6322143.160462  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 8364612.319489  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 9012017.616681  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 7187908.728915  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 7359812.140030  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7285196.092933  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 10036145.587545  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 7090981.637943  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 9 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.124836\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 9 tested\n",
      "\n",
      "Epoch 10 started\n",
      "loss: 8353913.251967  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 6472189.012530  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 7902212.043643  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 6825588.208922  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 6493907.731978  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 7359189.605090  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 7220333.450572  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 6432242.833510  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 7328629.784114  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 7952057.217316  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 7477355.744926  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7265435.556992  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 6616769.328565  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 6551027.036071  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 7587363.568123  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 7771613.189604  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 8028384.637868  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 8435757.500791  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 8916179.105336  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 6076405.784088  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 6391512.272170  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7635687.406968  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 7557845.824258  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 7443774.016374  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 6905508.032483  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 6335259.147608  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 6127122.005136  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 7361364.939569  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 8473300.054279  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 7969788.663634  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 6603896.218198  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 6172031.457630  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 7904534.176365  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 7495614.574231  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 6570549.777846  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 5942993.242180  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 8345367.738645  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 6963831.042061  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 10276706.124559  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 7327205.583036  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 7652692.304828  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 8171696.985069  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 9426369.005640  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 7981085.087433  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 5999478.344430  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 9573662.034894  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 7960396.597853  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 7105159.976614  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 10 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.124091\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 10 tested\n",
      "\n",
      "Model by epoch 10 saved\n",
      "\n",
      "Epoch 11 started\n",
      "loss: 7367746.050488  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 6369703.824065  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 8443747.600034  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 6057203.667416  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 6428948.044405  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 7922673.905264  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 6515430.133043  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7509486.192277  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 8648538.238729  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 6466757.520351  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 6762945.285876  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 6779041.021905  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 6857957.090702  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 6732311.138792  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 7416809.022521  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 6505369.101542  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 7343657.304396  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 9813956.733513  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 8219481.522336  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 8164738.050656  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 8285657.449697  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7805131.893769  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 7379962.488234  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 6587420.972762  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 6454721.012535  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 6610687.607789  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7229673.313330  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 7405569.554420  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 8447485.154642  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 6309059.753196  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 6000252.005008  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 6899536.792873  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 7421145.208462  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 10396839.957448  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 7543639.891901  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 8117589.058840  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 7589702.071340  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 9708142.724695  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 7276982.317925  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 7978790.298500  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 6828022.709198  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 9386294.941780  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 6459411.316875  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 6238072.410993  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 7172822.300305  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 6991487.543514  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 6403589.273421  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 11402576.749920  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 11 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.124872\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 11 tested\n",
      "\n",
      "Epoch 12 started\n",
      "loss: 7948694.225935  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 7496974.296268  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 7312879.595470  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 8455276.540938  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 7221723.863703  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 7001362.480596  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 7512780.253547  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7836292.677440  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 7368238.602690  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 6725052.647417  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 6065956.864420  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7318890.897815  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 7746326.559015  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 6830015.434153  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 7847742.546533  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 8143837.941566  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 10332708.355617  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 6418548.020811  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 6741288.626770  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 7320412.684474  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 5558141.356062  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7724162.079795  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 8349735.904597  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 7513796.217848  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 6381425.725947  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 8241884.332741  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7046789.407005  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 7832032.522135  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 7987339.149699  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 7574653.398593  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 6489909.635927  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 7145936.625569  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 6984287.831914  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 8996514.112108  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 6473753.880395  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 6431991.629597  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 10246146.677772  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 7486670.205831  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 11484486.862630  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 7028569.536579  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 8445967.502365  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 10376948.328738  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 6677162.305493  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 10547823.515853  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 6095362.338756  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7507102.453507  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 7593768.259440  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 9627760.635043  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 12 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.123640\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 12 tested\n",
      "\n",
      "Epoch 13 started\n",
      "loss: 10793066.550512  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 7321647.566105  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 7006492.684373  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 6689015.155176  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 7305905.160086  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 6888093.815906  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 6865438.140974  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 6494315.275781  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 6888757.190901  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 7305195.132648  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 6974582.104863  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 9944626.353286  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 8796356.666637  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 6814028.456870  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 6569271.696748  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 6945157.896894  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 7811653.350255  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 7587407.146849  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 6903084.596602  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 7793574.246031  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 7226487.489013  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7482107.395684  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 6994535.246529  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 8542779.515697  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 7277838.261872  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 7055292.766674  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 5603855.306386  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 7966530.535974  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 7358128.995238  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 7208636.548754  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 7689820.363686  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 7331150.056161  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 10417437.925980  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 6476210.832844  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 9445297.784283  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 7251839.773511  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 8923223.984282  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 7910740.367136  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 7991594.532912  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 8243070.342113  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 6671846.688965  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 7989467.066888  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 7340336.250878  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 7395402.951802  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 6976162.872065  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7573615.328571  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 11835464.777327  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 7646730.328463  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 13 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.123951\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 13 tested\n",
      "\n",
      "Epoch 14 started\n",
      "loss: 6224069.462278  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 7394211.482954  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 6560552.702833  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 6576363.744273  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 6063857.153915  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 6989009.745975  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 6927943.446658  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 6651303.092737  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 6279825.743335  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 8679299.207938  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 6350080.588470  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7110763.733942  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 7692868.509083  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 7499592.099948  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 5795519.760295  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 5588166.493055  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 6880808.306788  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 7626897.719987  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 7024938.158014  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 7331404.283135  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 6742969.640014  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7214191.837680  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 6953274.547492  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 6802619.733507  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 7139328.418658  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 7403185.399238  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 9709601.883035  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 6245490.047532  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 7440501.622836  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 8066283.375570  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 6289637.595428  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 7000581.186787  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 6564470.649447  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 8111907.646824  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 6980916.553465  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 7437441.427420  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 7356410.779947  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 6619268.248720  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 7169973.208216  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 6992865.352719  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 6612134.760649  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 10471118.892951  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 7370637.613396  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 7303681.357768  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 7066729.509810  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7006896.565840  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 7066063.382149  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 8961768.591080  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 14 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.122420\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 14 tested\n",
      "\n",
      "Epoch 15 started\n",
      "loss: 7354441.786047  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 6661317.995610  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 8901405.241950  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 9884554.851754  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 7592285.127590  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 6300101.547180  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 6707094.017645  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 6375102.174268  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 7365345.056605  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 9421997.599330  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 9197415.040521  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 6536756.010171  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 7113788.508054  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 7284968.590699  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 10232017.980408  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 6445319.581235  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 6938229.435549  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 7247585.383370  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 6782061.535090  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 6044642.135191  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 8203590.326698  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7742962.058743  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 10866912.379683  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 8344861.719125  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 6266401.441192  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 8451238.974929  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7354311.851369  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 7649707.916985  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 6983131.928756  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 7051086.905621  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 8902328.619746  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 7440750.796810  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 6219921.596847  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 7559851.068621  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 7493183.768229  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 6910312.897524  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 7418823.547503  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 7874515.925062  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 6853438.646034  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 7416832.203457  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 6724193.620543  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 9088692.165744  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 6458650.165204  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 7402791.976425  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 7672647.504946  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 6173551.783358  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 8052919.725085  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 6379840.500758  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 15 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.124098\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 15 tested\n",
      "\n",
      "Epoch 16 started\n",
      "loss: 6746953.617642  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 7631883.344660  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 6960506.257347  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 7105888.043855  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 7067114.807158  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 7455491.734712  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 7659976.326648  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 6706677.746212  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 7862423.157283  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 7050564.254406  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 9696431.697796  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7426363.644575  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 10217547.837756  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 6036783.808157  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 7589240.113212  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 6545830.918062  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 7893598.542862  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 8439569.460464  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 8893533.209604  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 6656531.410184  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 7088906.740935  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 9579148.666648  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 7138483.178368  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 7852468.122663  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 7439324.377813  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 7008305.193030  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 9175470.702927  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 6413207.298427  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 7549207.696751  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 8301704.961041  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 7808057.456909  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 6614410.173312  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 6961306.564345  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 6163427.338419  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 8343230.964454  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 7302468.365903  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 7503190.629988  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 7514039.787274  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 7435347.085968  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 8520940.471975  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 6679931.050958  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 7830296.703516  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 7668794.570045  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 6374163.927251  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 7681135.978194  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 6707139.978201  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 7618201.863518  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 6893184.721233  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 16 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.123859\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 16 tested\n",
      "\n",
      "Epoch 17 started\n",
      "loss: 7140278.013916  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 8417443.610991  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 7807423.200561  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 6540503.508382  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 7486484.879979  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 7534709.122964  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 6652696.882677  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7326040.208191  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 7622504.870992  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 6811780.766769  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 8159271.094122  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 6751682.125703  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 8859115.906956  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 7049538.256253  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 7181245.189150  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 7367804.246218  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 7608277.551023  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 6463710.295355  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 6635046.806994  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 6747494.916629  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 7721046.143616  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7262179.565048  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 6227667.569504  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 9199893.506519  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 6353862.342756  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 6894888.093428  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7957794.535709  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 7807976.537635  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 7499365.158623  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 11263353.380802  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 6962346.576440  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 8533080.585087  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 8356124.012792  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 6172705.472730  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 7131601.209514  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 6773080.752522  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 9649055.294154  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 9453159.602034  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 7650853.369762  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 7059641.920275  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 7529010.798013  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 7733310.395492  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 9241819.352509  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 7046927.569142  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 6138408.617601  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 8394212.065718  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 11586290.661849  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 7732690.622154  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 17 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.124483\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 17 tested\n",
      "\n",
      "Epoch 18 started\n",
      "loss: 6479314.543879  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 5926420.300076  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 10687800.643105  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 7785088.868277  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 6578112.904367  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7439693.389248  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 8659325.422474  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 6507545.648063  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 5742993.281178  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 6939220.557255  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 8614372.405067  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 8055093.326140  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 7560854.646934  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 7344598.231635  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 7352950.521671  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 6926397.781005  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 5928855.227494  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 6972130.680483  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 6600233.678211  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 8762727.912876  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 7147896.551732  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 6442619.234420  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 6897253.211843  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 7183264.556591  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 6055271.798746  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 6440394.197664  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7425092.173294  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 7801120.160097  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 6889581.801206  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 6235703.247453  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 7900635.302911  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 6195117.741969  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 7692397.043000  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 6901822.611765  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 7534896.159947  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 7558469.329765  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 6617081.820018  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 8572760.924670  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 6645239.426967  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 8282914.414318  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 6287744.429528  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 7489540.852629  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 7034675.630366  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 5122449.824585  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 7108224.227182  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 11719907.033504  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 6673826.816265  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 6343817.490613  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 18 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.124124\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 18 tested\n",
      "\n",
      "Epoch 19 started\n",
      "loss: 6524524.306317  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 7091429.493748  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 9122415.167635  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 8739438.026384  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 5851707.593636  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 6451415.541112  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 7277338.502048  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7480202.376416  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 7544221.572368  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 6446811.244737  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 5794825.810134  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7452301.351994  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 7249482.556439  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 8563479.083287  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 6138697.195174  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 7015127.612971  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 6445562.451971  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 6931735.662103  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 6415080.892911  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 6511950.211271  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 5725950.444237  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7717194.423050  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 7009848.830076  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 7703041.731668  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 7270142.263285  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 7007592.071160  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 11717529.427623  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 6829485.628953  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 6621562.252451  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 6607834.645655  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 8433438.098304  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 8809978.974541  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 7710586.453188  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 7589125.325994  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 7866032.892982  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 9257299.040830  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 6833146.349605  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 7533799.892875  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 8643681.504760  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 6139246.954411  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 5468862.605959  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 8072151.036403  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 6266704.560698  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 6161975.505906  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 7409426.420538  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 8160684.868378  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 7480956.915583  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 9801758.996875  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 19 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.123712\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 19 tested\n",
      "\n",
      "Epoch 20 started\n",
      "loss: 7084924.986658  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 8714011.383863  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 7881652.396750  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 7080219.391389  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 6538751.733611  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 6353057.901956  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 6699256.318218  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7022749.405937  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 5236735.345967  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 9507091.678988  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 7716546.594207  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7058569.918341  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 9006935.305863  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 6440649.667827  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 7575845.274971  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 7803972.149897  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 11256963.751465  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 7142979.734593  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 6980832.900191  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 6537380.916059  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 7310233.126724  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 8703547.533693  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 6864427.360587  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 6047189.504359  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 7291337.231028  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 6453782.127278  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 10481556.945835  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 6694043.659367  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 7399349.383866  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 6940611.980997  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 7758751.016663  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 7685650.870739  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 6815621.893490  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 7281448.467744  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 5945476.378199  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 7786861.805899  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 7087101.763601  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 7606549.628651  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 8065423.024243  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 8422793.821804  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 7751269.402649  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 7701803.991920  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 6867784.956370  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 7961326.445126  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 7548519.499701  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7316054.872019  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 7828838.661189  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 6995807.988147  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 20 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.121911\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 20 tested\n",
      "\n",
      "Model by epoch 20 saved\n",
      "\n",
      "Epoch 21 started\n",
      "loss: 7691537.071958  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 7141197.407698  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 7539119.600297  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 8898318.834548  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 11170849.705765  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 5717959.479662  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 7254901.120435  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 5764782.499496  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 7928991.234779  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 8169596.847853  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 7488968.307231  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7143528.916041  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 7598930.842905  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 7809646.726581  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 9677301.820092  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 7970477.315486  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 7338138.472197  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 7276719.596114  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 6658655.095012  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 7405712.633042  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 6821304.851362  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 6639326.886704  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 7233022.379869  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 7971403.523504  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 9580609.998281  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 10088026.666568  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 6233633.764656  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 7684401.508557  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 8083579.949112  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 8146014.569268  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 7452121.319713  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 11195228.949997  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 10865954.933746  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 7916191.028415  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 7319329.198077  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 7690671.712457  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 6494525.153853  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 6573583.572448  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 8366407.130639  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 7756734.590349  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 7068918.125257  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 7758386.944453  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 7201526.983784  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 7493102.823485  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 6937000.804118  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 6246463.194509  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 10499333.082453  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 6303822.664983  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 21 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.123785\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 21 tested\n",
      "\n",
      "Epoch 22 started\n",
      "loss: 7145700.980294  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 7320163.814421  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 7284337.215029  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 7081221.744658  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 7091235.685584  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 6367926.877634  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 7563826.013230  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 6156950.731593  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 7273483.876556  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 7065069.024833  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 7257319.237755  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7172599.815520  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 7667104.373203  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 7250063.671281  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 6597114.250344  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 8645116.303083  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 6239510.707504  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 6987755.056154  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 6471588.161817  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 7571321.329859  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 6783340.811472  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7318669.085394  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 6019144.906289  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 7671497.282477  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 6978766.139309  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 7356785.884017  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7671146.166919  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 5277312.229134  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 7320569.360863  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 7542847.102692  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 6392557.163573  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 10442150.407252  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 8437315.667770  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 9243941.487230  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 9490031.542125  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 7362073.345871  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 6016348.008432  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 6488267.523930  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 7258070.121387  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 8017788.474290  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 7898728.678085  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 6839103.046616  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 11328499.529045  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 6865075.648328  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 7407840.165253  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7834573.635265  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 7118452.185210  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 7578051.682477  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 22 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.123251\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 22 tested\n",
      "\n",
      "Epoch 23 started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7876555.964792  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 7403364.159092  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 6827324.788428  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 8490884.974644  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 7842232.715878  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 7239753.226328  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 7349774.212884  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7330005.198412  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 6888562.263424  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 7853108.333452  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 8767648.378580  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7190905.947018  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 7220346.807355  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 9231135.786626  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 7686401.527747  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 6813582.484979  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 7204531.489676  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 8126166.570968  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 7069484.227822  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 7407917.077243  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 7195603.667121  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7292467.644537  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 8113307.045332  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 6590785.678826  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 6898809.867650  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 6585316.758062  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7572780.410322  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 6712439.484709  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 8007961.618194  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 7612645.463557  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 7638864.949894  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 7419619.896974  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 7529986.410755  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 8896115.955044  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 7915377.829007  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 8485880.778657  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 7517929.812328  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 7753860.742487  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 7629792.686805  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 6607416.465154  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 8754165.666704  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 7388872.232793  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 7157073.605336  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 9783140.781519  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 7294997.712054  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 6254711.663449  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 8780980.334563  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 6948553.817989  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 23 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.124475\n",
      " Avg discriminator loss: 1.992622\n",
      "\n",
      "Epoch 23 tested\n",
      "\n",
      "Epoch 24 started\n",
      "loss: 6740622.602878  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 6809590.678361  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 7252261.714193  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 7455415.067833  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 6895773.248756  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 7309216.442973  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 9376948.659852  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7331861.225304  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 6718285.650356  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 6617574.447948  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 6997170.371018  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7742008.038707  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 7549647.376167  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 6886999.690110  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 7068322.023132  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 6780385.341296  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 8883556.890390  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 7425941.148839  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 7673464.852718  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 7216066.604295  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 7557045.732348  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 5207319.741843  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 7188359.543382  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 7888945.914269  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 7688226.841524  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 7523781.001345  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7666068.767973  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 6886182.855920  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 7650825.605707  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 5187572.919593  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 5233982.278443  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 7391458.126143  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 7017861.050572  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 6289502.759872  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 6249187.209626  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 6189329.687765  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 7953013.224960  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 7305465.169885  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 6990779.709210  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 6462056.991493  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 7701450.924491  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 6815990.731685  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 6311408.092257  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 7828854.554597  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 7105666.243126  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7052247.782476  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 5492664.367847  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 9495334.450660  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 24 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.124974\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 24 tested\n",
      "\n",
      "Epoch 25 started\n",
      "loss: 5877944.343851  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 9730502.790713  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 7034363.630457  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 7562252.826379  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 6785022.035038  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 9100833.198370  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 7314746.433867  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7270356.811435  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 10038851.601113  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 6811176.403801  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 6109438.937598  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 6692216.594334  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 7271866.448819  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 7533085.529398  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 7211926.710261  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 6005382.896414  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 6241557.030857  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 7235977.434134  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 7843095.468640  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 7855833.965886  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 6409005.310597  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 11650814.583813  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 7263709.658878  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 6752602.983859  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 5765454.748389  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 6600137.291554  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7848739.009521  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 8210174.557359  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 8159730.032655  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 7451813.578942  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 7207130.894407  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 6698786.098384  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 7432020.847163  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 6396174.986743  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 6811733.775160  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 7082888.902694  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 11281607.023456  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 6331082.576629  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 7961908.672729  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 7214021.729943  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 5785652.392497  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 7581821.860687  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 8793101.244809  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 8854211.073594  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 6828008.444719  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 8507932.800871  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 6402956.665054  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 6835394.816547  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 25 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.121659\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 25 tested\n",
      "\n",
      "Epoch 26 started\n",
      "loss: 7777750.743447  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 5995643.920039  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 6776539.999102  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 7684010.033548  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 7524532.705707  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 7887516.170391  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 7504824.626743  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 6918737.678454  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 5544618.282560  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 6488828.766794  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 8576850.970395  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 6583887.757038  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 6938478.167409  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 7054166.681895  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 7255448.840239  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 7847092.412675  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 8304514.703341  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 7371979.258305  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 7233745.709002  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 6176086.683578  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 6777092.065058  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7595674.911147  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 8120932.168564  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 8353662.738661  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 7952346.945121  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 7865212.572727  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 6719375.357574  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 7560359.318975  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 7204980.373582  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 7847414.246781  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 6346722.120134  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 6836153.184997  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 8005873.967411  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 7267741.333159  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 9572852.980834  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 6873513.550325  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 7178512.406447  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 8264979.099613  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 7515062.225217  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 8815740.388836  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 10956344.428026  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 7650669.764865  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 7823603.327441  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 6541835.291322  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 6622651.465440  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7362672.236146  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 7216938.649276  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 8112193.582114  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 26 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.123602\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 26 tested\n",
      "\n",
      "Epoch 27 started\n",
      "loss: 8402320.702729  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 7420588.522143  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 6548066.777996  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 7148762.136437  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 7202479.024590  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 9902023.696949  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 7968217.770244  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 6065757.335275  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 7105054.649710  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 6996961.340698  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 6445884.569010  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 11079996.946169  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 7226053.314419  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 8270315.460975  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 6337629.465279  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 7228329.050992  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 6585208.590598  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 7409163.689660  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 8654453.676706  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 8071476.236155  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 4890362.636726  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 7619151.950192  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 7811311.904523  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 6451743.384041  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 6311409.276382  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 11155801.868164  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 9167048.185421  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 7108442.054618  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 9271372.712187  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 7030732.291041  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 7718837.745233  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 8599203.129002  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 7069989.137629  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 7340422.969081  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 6483864.444806  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 7425229.753195  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 7798424.223064  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 6806523.209776  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 6209553.275273  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 7830062.809172  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 7851080.043154  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 8362022.936212  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 7232901.087834  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 7647584.322957  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 9858365.071904  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7726374.943456  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 5991273.504179  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 7003835.159065  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 27 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.124328\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 27 tested\n",
      "\n",
      "Epoch 28 started\n",
      "loss: 6248729.625138  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 6713698.985745  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 9261546.214182  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 7807348.408366  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 6718766.225941  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 7532836.037712  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 10122218.900058  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7756084.850226  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 5946305.271995  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 7887797.931253  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 7882159.290468  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7585846.042536  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 6051207.463448  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 7756112.044437  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 7349603.683703  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 7596721.301918  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 6608167.155753  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 6705827.551718  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 7556557.924602  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 9378254.293925  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 7666116.235330  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 6140503.956994  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 6363327.568035  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 7602827.373616  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 7048359.410916  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 8393435.731595  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 7130153.862182  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 7121749.764581  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 7920326.843790  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 6570097.861454  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 8205720.371932  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 6489813.115416  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 6518489.941583  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 6143547.704876  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 7611409.251461  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 7445839.195709  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 6850735.226715  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 7270520.700062  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 7226988.758595  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 8394197.840195  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 6006195.649671  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 6807594.212962  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 7463243.830018  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 7321567.268827  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 7056825.968496  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7739581.075280  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 7130677.048949  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 6496165.578508  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 28 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.123832\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 28 tested\n",
      "\n",
      "Epoch 29 started\n",
      "loss: 7677312.223107  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 12248703.091805  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 7397931.117879  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 7311401.517390  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 6380452.630401  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 7398706.885915  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 7154283.903643  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7143069.278057  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 10904589.302994  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 6386918.115654  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 7198134.910359  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7173927.973295  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 6355606.852003  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 7504698.762936  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 10539875.142738  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 8392033.539711  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 6723858.867367  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 10049328.558431  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 7092790.483315  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 7599885.158693  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 8646552.016967  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 6438797.881392  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 6746242.919716  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n",
      "loss: 7752901.662070  [  960/ 1924]\tloss: 82.893066  [  960/ 1924]\n",
      "loss: 7071195.554510  [ 1000/ 1924]\tloss: 82.893066  [ 1000/ 1924]\n",
      "loss: 7014583.935662  [ 1040/ 1924]\tloss: 82.893066  [ 1040/ 1924]\n",
      "loss: 8165741.338277  [ 1080/ 1924]\tloss: 82.893066  [ 1080/ 1924]\n",
      "loss: 7560496.085224  [ 1120/ 1924]\tloss: 82.893066  [ 1120/ 1924]\n",
      "loss: 7285531.060645  [ 1160/ 1924]\tloss: 82.893066  [ 1160/ 1924]\n",
      "loss: 7512890.727128  [ 1200/ 1924]\tloss: 82.893066  [ 1200/ 1924]\n",
      "loss: 7416720.879058  [ 1240/ 1924]\tloss: 82.893066  [ 1240/ 1924]\n",
      "loss: 7548857.274800  [ 1280/ 1924]\tloss: 82.893066  [ 1280/ 1924]\n",
      "loss: 9255010.277655  [ 1320/ 1924]\tloss: 82.893066  [ 1320/ 1924]\n",
      "loss: 7065375.805167  [ 1360/ 1924]\tloss: 82.893066  [ 1360/ 1924]\n",
      "loss: 6222891.768467  [ 1400/ 1924]\tloss: 82.893066  [ 1400/ 1924]\n",
      "loss: 6604417.263115  [ 1440/ 1924]\tloss: 82.893066  [ 1440/ 1924]\n",
      "loss: 9504994.124061  [ 1480/ 1924]\tloss: 82.893066  [ 1480/ 1924]\n",
      "loss: 7198341.255609  [ 1520/ 1924]\tloss: 82.893066  [ 1520/ 1924]\n",
      "loss: 6739733.845888  [ 1560/ 1924]\tloss: 82.893066  [ 1560/ 1924]\n",
      "loss: 6227693.171028  [ 1600/ 1924]\tloss: 82.893066  [ 1600/ 1924]\n",
      "loss: 7177562.600300  [ 1640/ 1924]\tloss: 82.893066  [ 1640/ 1924]\n",
      "loss: 6638985.809735  [ 1680/ 1924]\tloss: 82.893066  [ 1680/ 1924]\n",
      "loss: 7513947.724056  [ 1720/ 1924]\tloss: 82.893066  [ 1720/ 1924]\n",
      "loss: 7108695.047368  [ 1760/ 1924]\tloss: 82.893066  [ 1760/ 1924]\n",
      "loss: 6416430.016345  [ 1800/ 1924]\tloss: 82.893066  [ 1800/ 1924]\n",
      "loss: 7142307.209400  [ 1840/ 1924]\tloss: 82.893066  [ 1840/ 1924]\n",
      "loss: 7202963.120173  [ 1880/ 1924]\tloss: 82.893066  [ 1880/ 1924]\n",
      "loss: 6627037.930418  [ 1920/ 1924]\tloss: 82.893066  [ 1920/ 1924]\n",
      "Epoch 29 trained\n",
      "Test Error:\n",
      " Avg generator loss: 1845224.123721\n",
      " Avg discriminator loss: 62.169800\n",
      "\n",
      "Epoch 29 tested\n",
      "\n",
      "Epoch 30 started\n",
      "loss: 5726507.907594  [   40/ 1924]\tloss: 82.893066  [   40/ 1924]\n",
      "loss: 7094330.373475  [   80/ 1924]\tloss: 82.893066  [   80/ 1924]\n",
      "loss: 7806231.752793  [  120/ 1924]\tloss: 82.893066  [  120/ 1924]\n",
      "loss: 7179846.269764  [  160/ 1924]\tloss: 82.893066  [  160/ 1924]\n",
      "loss: 7859812.328928  [  200/ 1924]\tloss: 82.893066  [  200/ 1924]\n",
      "loss: 6930666.978181  [  240/ 1924]\tloss: 82.893066  [  240/ 1924]\n",
      "loss: 9880949.762049  [  280/ 1924]\tloss: 82.893066  [  280/ 1924]\n",
      "loss: 7474098.164401  [  320/ 1924]\tloss: 82.893066  [  320/ 1924]\n",
      "loss: 7157796.689959  [  360/ 1924]\tloss: 82.893066  [  360/ 1924]\n",
      "loss: 6967841.819132  [  400/ 1924]\tloss: 82.893066  [  400/ 1924]\n",
      "loss: 8173572.673127  [  440/ 1924]\tloss: 82.893066  [  440/ 1924]\n",
      "loss: 7048838.598163  [  480/ 1924]\tloss: 82.893066  [  480/ 1924]\n",
      "loss: 6595449.404738  [  520/ 1924]\tloss: 82.893066  [  520/ 1924]\n",
      "loss: 11160795.216484  [  560/ 1924]\tloss: 82.893066  [  560/ 1924]\n",
      "loss: 8019053.532783  [  600/ 1924]\tloss: 82.893066  [  600/ 1924]\n",
      "loss: 7950092.853527  [  640/ 1924]\tloss: 82.893066  [  640/ 1924]\n",
      "loss: 6099374.095643  [  680/ 1924]\tloss: 82.893066  [  680/ 1924]\n",
      "loss: 7493798.879531  [  720/ 1924]\tloss: 82.893066  [  720/ 1924]\n",
      "loss: 7547841.263278  [  760/ 1924]\tloss: 82.893066  [  760/ 1924]\n",
      "loss: 10013136.486475  [  800/ 1924]\tloss: 82.893066  [  800/ 1924]\n",
      "loss: 7140975.558816  [  840/ 1924]\tloss: 82.893066  [  840/ 1924]\n",
      "loss: 8111321.359087  [  880/ 1924]\tloss: 82.893066  [  880/ 1924]\n",
      "loss: 6644408.438388  [  920/ 1924]\tloss: 82.893066  [  920/ 1924]\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1} started\")\n",
    "    train_loop(\n",
    "        generator,\n",
    "        generator_train_dataloader,\n",
    "        generator_loss,\n",
    "        generator_optimiser,\n",
    "        discriminator,\n",
    "        discriminator_train_dataloader,\n",
    "        discriminator_loss,\n",
    "        discriminator_optimiser\n",
    "    )\n",
    "    print(f\"Epoch {t + 1} trained\")\n",
    "    test_loop(\n",
    "        generator,\n",
    "        generator_test_dataloader,\n",
    "        generator_loss,\n",
    "        discriminator,\n",
    "        discriminator_test_dataloader,\n",
    "        discriminator_loss\n",
    "    )\n",
    "    print(f\"Epoch {t + 1} tested\\n\")\n",
    "    if (t + 1) % 10 == 0:\n",
    "        torch.save(generator.state_dict(), os.path.join('generator.bin'))\n",
    "        torch.save(discriminator.state_dict(), os.path.join('discriminator.bin'))\n",
    "        print(f\"Model by epoch {t + 1} saved\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
